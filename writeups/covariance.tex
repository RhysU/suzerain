\documentclass[letterpaper,11pt,nointlimits,reqno]{amsart}

% Packages
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{fancyhdr}
\usepackage{fullpage}
\usepackage{lastpage}
\usepackage{mathrsfs}
\usepackage{mathtools}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{parskip}
\usepackage{setspace}
\usepackage{txfonts}
\usepackage{varioref}

\usepackage[obeyDraft,textsize=scriptsize]{todonotes}

% Hyperref package must be last otherwise the contents are jumbled
% hypertexnames disabled to fix links pointing to incorrect locations
\usepackage[hypertexnames=false,final]{hyperref}

\mathtoolsset{showonlyrefs,showmanualtags}
\allowdisplaybreaks[1] % Allow grouped equations to be split across pages

% Line Spacing
\singlespacing

% Set appropriate header/footer information on each page
\fancypagestyle{plain}{
    \fancyhf{}
    \renewcommand{\headheight}{2.0em}
    \renewcommand{\headsep}{0.75em}
    \renewcommand{\headrulewidth}{1.0pt}
    \renewcommand{\footrulewidth}{0pt}
    \lhead{
        Covariance estimation from thinning nonuniform, autocorrelated samples.
    }
    \rhead{
        Page \thepage{} of \pageref{LastPage}
    }
}
\pagestyle{plain}

% Document-specific commands
\newcommand{\trans}[1]{{#1}^{\ensuremath{\mathsf{T}}}}
\newcommand{\OO}[1]{\operatorname{O}\left(#1\right)}
\DeclareMathOperator{\cov}{cov}

\begin{document}

\subsection*{Problem}

Assume the existence of two stationary, real-valued stochastic processes
$\vec{\mathscr{X}}$ and $\vec{\mathscr{Y}}$ with finite, but unknown and
possibly distinct, first and second moments and decaying, but unknown and
possibly distinct, autocorrelations.  Estimate
$\cov\left(\vec{\mathscr{X}},\vec{\mathscr{Y}}\right)$ given two realization
sequences $\vec{x}\left(t_i\right)$ and $\vec{y}\left(t_i\right)$ for
$i\in\left\{0,1,\dots,N-1\right\}$ where $t_i < t_{i+1}$.

\subsection*{Assumptions}

Data was sampled across times appreciably larger than the decay of the
autocorrelation of both processes and $I$ ``covers $\left[0,t\right)$
well-enough'' that interrogating $\alpha \sqrt{N}$ consecutive samples for some
fixed $1\le\alpha\le\sqrt{N}$ will provide adequate autocorrelation information
without running afoul of the Nyquist criterion.  $N$ must be large enough that
sample means of $\vec{x}$ and $\vec{y}$ may be treated as known means without
incurring too awful a bias \citep[see][]{Percival1993Three}.  The nonuniform
sampling permits missing samples or sample time drift across a long-running
simulation.  Ideally, samples will be collected uniformly in time.

\subsection*{Approach}

The procedure has four steps.  First, construct uniform temporal samples
$\vec{X}$ and $\vec{Y}$ from the nonuniform $\vec{x}$ and $\vec{y}$ via a
Fourier-based projection.  Second, estimate the cross-spectra of $\vec{X}$ and
$\vec{Y}$ following \citet{Welch1967Use} to maximally reduce the variance of
these estimates given limited data.  Third, estimate the covariances from the
cross-spectra and compute an effective sample size $N'$ per recommendations by
\citet{Thiebaux1984Interpretation}.  Finally, compute the sample covariance of
$\vec{x}_i$ and $\vec{y}_i$ thinned to contain only $N'$ effectively i.i.d.
samples.  This thinned sample covariance, not the estimate based on the
projections, is the output.

\subsection*{Projection onto uniform temporal samples}

Set $t_0=0$ without loss of generality and define
$$
T = \frac{t_{N-1} N}{N-1}.
$$
$T$ will be the sampling duration for a non-uniform discrete transform (NDFT).
It was chosen so that a subsequent uniform, inverse discrete Fourier transform
(IDFT) will exactly recover $\vec{X}=\vec{x}$ when $\vec{x}$ was sampled
uniformly in time.  The temporal projection performs an NDFT followed by an
IDFT to produce $N$ uniform samples $\vec{X}\left(2\pi{}i/N\right)$ from the
original $\vec{x}\left(t_k\right)$.  $\vec{Y}$ is constructed identically from
$\vec{y}$.

The projection $A$ operates separately on each element $x$ of $\vec{x}$
to produce each element $X$ of $\vec{X}$ according to
$$
    X\left(2\pi{}i/N\right)
    =
    A_{ik} x\left(t_k\right)
    =
    \frac{1}{N}
    \sum_{j=0}^{N-1} \exp\left(\frac{ 2\pi\sqrt{-1} i j   }{N} \right)
    \sum_{k=0}^{N-1} \exp\left(\frac{-2\pi\sqrt{-1} j t_k }{T} \right)
    x\left(t_k\right)
.
$$
As $t_k$ are not known here, the usual discrete orthogonality relations are not
used to simplify $A$.  Fast algorithms may be used to compute the NDFT but will
incur more floating point error than the naive NDFT
\citep[see][]{Kunis2008Time}.  Hermitian symmetry can be used to reduce the
cost of both the NDFT and IDFT.  The interpolation errors near $t_0$ and
$t_{N-1}$ should be assuaged by windowing during the cross-spectra estimation.

\subsection*{Cross-spectra estimation from the uniform samples}

An estimate of the cross-spectra $\hat{P}_{XY}$ is computed for each pair in
$\vec{X} \otimes \vec{Y}$ using Welch's approach~\citep{Welch1967Use} to the
method of smoothed periodograms~\citep{Bartlett1948Smoothing}.  Only the ``zero
mode'' $\hat{P}_{XY}(0)$ will be of later interest.  Its variance is twice that
of the rest of $\hat{P}_{XY}$ as Welch warns.

Procedurally, Welch's averaging segment length $L$ should be taken as
$\alpha\sqrt{N}$ for some $1\le\alpha\le\sqrt{N}$.  A fraction of $\sqrt{N}$ is
chosen so the $N\to\infty$ limit might possibly be asymptotically unbiased (as
mentioned by \citet[p. 803]{Thiebaux1984Interpretation}).  To get a near
maximum reduction in the covariance from a fixed number of samples, the
segments should be overlapped by one half their length (i.e.  $D = L / 2 =
\alpha{}N$).  The spectral window that Welch denotes $W_1$ is suggested though
with either $W_1$ or $W_2$ should give comparable results.

The free parameter $\alpha$ plays a role similar to the ``coarse-graining''
within the variance estimation procedure described in the appendix of
\citet{Hoyas2008Reynolds}.  When applied to adequate data it is expected that
taking $\alpha\to\sqrt{N}$ will cause $\hat{P}_{XY}(0)$ to converge.

Segmented averaging, or some other periodogram smoothing, is recommended by
\citet[\textsection{}17.1.3]{Storch2001Statistical} when the number of samples
is moderate or small.  \citet[\textsection{}12.3.7]{Storch2001Statistical}
warns that periodogram estimation is inappropriate for investigating the ``zero
mode'' and suggests an approach following \citet{Madden1976Estimates} wherein
the sample mean is removed and then the ``zero mode'' extrapolated.  Another
suggested by \citet[\textsection{}17.1.3]{Storch2001Statistical} is fitting a
stochastic process for the purposes of estimating its characteristic
decorrelation time\todo{Discrete AR(p) with non-constant step size?  Continuous
AR(p)?}.  Rather than segmented averaging, semantically-equivalent
autocorrelation smoothing may be more appropriate as hinted at by
\citet{Scargle1982Studies} and detailed by \citet{Richards1967Computing}.

\subsection*{Covariance estimation from the cross-spectra estimates}

In the limit of infinite, uniform samples the zero frequency values from exact
element-by-element cross-spectra are nothing but $\cov\left( \vec{\mathscr{X}}
, \vec{\mathscr{Y}} \right)$ by the Wiener--Khinchin theorem.
$\hat{P}_{XY}(0)$ is used to estimate of the covariance of each pair of
elements.

Following \citet{Thiebaux1984Interpretation}, the effective sample size $N'$ is
defined by
$$
    N' = N \min \frac{\cov(x,y)} {\hat{P}_{XY}(0)}
$$
where the minimum is found over all elements in $\vec{x}\otimes\vec{y}$,
$\hat{P}_{XY}$ is taken for the corresponding pair within $\left(\vec{X}
\otimes \vec{Y} \right)$, and $\cov(x,y)$ is an unbiased sample covariance
computed assuming all original samples are independent (and therefore their
temporal spacing is irrelevant).  This single value $N'$ gauges the number of
independent samples present in the overall data given the estimated
autocorrelation structure.

\subsection*{Thinning to effectively i.i.d samples}

The original $N$ samples $\vec{x}(t_i)$ and $\vec{y}(t_i)$ should be thinned so
that only $N'$ samples, separated as maximally in time as possible, remain.
From those samples the classical unbiased covariance is computed.  This thinned
covariance is reported as the output of the procedure.  When
$\mathscr{X}=\mathscr{Y}$, the thinned sample variance should be reported
alongside the thinned sample mean.

\subsection*{Checking consistency and stationarity}

Small changes to the periodogram averaging segment factor $\alpha$ should cause
small perturbations to the results when $N$ is sufficiently large and the $t_i$
are adequately spaced.  This procedure might be amenable to the Geweke
diagnostic~\citep{Geweke1992Evaluating} as a way to assess stationarity.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%% Bibliography %%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand*{\doi}[1]{\href{http://dx.doi.org/\detokenize{#1}}{doi: #1}}
\bibliographystyle{plainnat}
\bibliography{references}

\end{document}
